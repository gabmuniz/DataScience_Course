# Teste do modelo
```{r}
test.probs <-predict(logit, test, type='response')
pred.logit <- rep('Down',length(test.probs))
pred.logit[test.probs>=0.5] <- 'Up'
table(pred.logit, test$Direction)
```
```{r}
confusionMatrix(table(test$Direction, pred.logit))
```
```{r}
modelFit<- train(Direction~Lag1+Lag2+Lag3, method='glm',preProcess=c('scale', 'center'), data=train, family=binomial(link='logit'))
summary(modelFit)
```
```{r}
confusionMatrix(test$Direction, predict(modelFit, test))
```
## Precisão geral de 59%, sendo 78% de precisão em dias de alta das ações.
## Os modelos defasados do mercado de ações não são o melhor indicador de desempenho.
# Split data into testing and training
train<-Smarket[Year<2005,]
test<-Smarket[Year==2005,]
# Split data into testing and training
train<-Smarket[Year<2005,]
require(knitr)
---
title: "R Notebook"
output:
html_document:
df_print: paged
---
# Regressão Logística
## Modelo para predizer quando um indivíduo ficará inadimplente com o cartão de crédito
```{r}
require(knitr)
```
```{r}
# Load the textbook R package
require(ISLR)
# Load in the credit data
attach(Default)
```
```{r}
str(Default)
```
## De 1.000 observações, notamos 4 variáveis.
## DÚVIDA: para estimar a quantidade de "default" (inadimplentes) abaixo, o cálculo realizado foi o número de (SIM / NÂO) * 100,e não (SIM / TOTAL) * 100?
```{r}
tmp <- table(default)
(tmp[[2]]/tmp[[1]])*100
```
## Segundo os resultados, temos ~3% de inadimplentes.
```{r}
library(ggplot2);
library(gridExtra)
```
```{r}
x <- qplot(x=balance, y=income, color=default, shape=default, geom='point')+scale_shape(solid=FALSE)
y <- qplot(x=default, y=balance, fill=default, geom='boxplot')+guides(fill=FALSE)
z <- qplot(x=default, y=income, fill=default, geom='boxplot')+guides(fill=FALSE)
# Plot
x
```
## De acordo com o gráfico, o preditor "balance" possui maior impacto sobre a inadimplência, representando tal tendência a partir do valor ~1500. O preditor "income" impacta em menor grau o fator inadimplência, havendo uma leve concentração de "default" na faixa de renda ~20.000 quando o "balance" ultrapassar o valor de ~1500.
```{r}
grid.arrange(y, z, nrow=1)
```
# Modelo Basico
```{r}
logit <- glm(default ~ balance, data=Default, family='binomial')
summary(logit)
```
## B1 = 0,005. - o "balance" está associado a um aumento na probabilidade de "default".
## Um aumento de uma unidade em "balance" aumenta as chances de "default" em 0,0055 unidades.
# Variáveis categóricas (qualitativas)
```{r}
# Create a new dummy variable for students
Default$studentD <- 0
Default$studentD[Default$student=="Yes"] <- 1
logit <- glm(default ~ studentD, data=Default, family='binomial')
summary(logit)
```
## Conclui que estudantes tem mais chance de "default".
## Um aumento de uma unidade em "Student" aumenta as chances de "default" em 0,4049 unidades.
## DÚVIDA 2 - última coluna - modelo é válido? como analisar?
# Regressão Logística Multipla
```{r}
logit <- glm(default ~ income + balance + studentD, family='binomial', data=Default)
summary(logit)
```
## DÚVIDA - Quanto é feita a análise multipla, os estudantes apresentam tendência menor a serem "Default" que os não estudantes.
## Logistic Regression for > 2 Response Classes
# Example 1: College Admissions - Unavailable
# Example 2: Stock Market Data
## Análise de 500 itens de estoque ao longo de 1250 dias entre 2001 - 2005.
```{r}
library(ISLR)
str(Smarket)
```
# Correlação menos a variável de direção:
```{r}
cor(Smarket[,-9])
```
## Observamos que não existe correlação entre os "lags" e a variável "Today", entretando há alta correlação entre "Volume" e "Year", indicando que o volume aumenta com o tempo
```{r}
attach(Smarket)
plot(Volume)
```
#Divisão entre treino e teste para regressão logistica
```{r}
# Split data into testing and training
train<-Smarket[Year<2005,]
test<-Smarket[Year==2005,]
```
```{r}
library(caret)
```
# Criação do modelo
```{r}
logit <- glm(Direction ~ Lag1+Lag2+Lag3, family='binomial', data=train)
summary(logit)
```
# Teste do modelo
```{r}
test.probs <-predict(logit, test, type='response')
pred.logit <- rep('Down',length(test.probs))
pred.logit[test.probs>=0.5] <- 'Up'
table(pred.logit, test$Direction)
```
```{r}
confusionMatrix(table(test$Direction, pred.logit))
```
```{r}
modelFit<- train(Direction~Lag1+Lag2+Lag3, method='glm',preProcess=c('scale', 'center'), data=train, family=binomial(link='logit'))
summary(modelFit)
```
```{r}
confusionMatrix(test$Direction, predict(modelFit, test))
```
## Precisão geral de 59%, sendo 78% de precisão em dias de alta das ações.
## Os modelos defasados do mercado de ações não são o melhor indicador de desempenho.
test.probs <-predict(logit, test, type='response')
pred.logit <- rep('Down',length(test.probs))
pred.logit[test.probs>=0.5] <- 'Up'
table(pred.logit, test$Direction)
logit <- glm(Direction ~ Lag1+Lag2+Lag3, family='binomial', data=train)
summary(logit)
train<-Smarket[Year<2005,]
test<-Smarket[Year==2005,]
train<-Smarket[Year<2005,]
test<-Smarket[Year==2005,]
require(knitr)
require(ISLR)
# Load in the credit data
attach(Default)
str(Default)
tmp <- table(default)
(tmp[[2]]/tmp[[1]])*100
library(ggplot2);
library(gridExtra)
x <- qplot(x=balance, y=income, color=default, shape=default, geom='point')+scale_shape(solid=FALSE)
y <- qplot(x=default, y=balance, fill=default, geom='boxplot')+guides(fill=FALSE)
z <- qplot(x=default, y=income, fill=default, geom='boxplot')+guides(fill=FALSE)
# Plot
x
grid.arrange(y, z, nrow=1)
logit <- glm(default ~ balance, data=Default, family='binomial')
summary(logit)
# Create a new dummy variable for students
Default$studentD <- 0
Default$studentD[Default$student=="Yes"] <- 1
logit <- glm(default ~ studentD, data=Default, family='binomial')
summary(logit)
logit <- glm(default ~ income + balance + studentD, family='binomial', data=Default)
summary(logit)
library(ISLR)
str(Smarket)
cor(Smarket[,-9])
attach(Smarket)
plot(Volume)
# Split data into testing and training
train<-Smarket[Year<2005,]
test<-Smarket[Year==2005,]
library(caret)
logit <- glm(Direction ~ Lag1+Lag2+Lag3, family='binomial', data=train)
summary(logit)
test.probs <-predict(logit, test, type='response')
pred.logit <- rep('Down',length(test.probs))
pred.logit[test.probs>=0.5] <- 'Up'
table(pred.logit, test$Direction)
confusionMatrix(table(test$Direction, pred.logit))
modelFit<- train(Direction~Lag1+Lag2+Lag3, method='glm',preProcess=c('scale', 'center'), data=train, family=binomial(link='logit'))
summary(modelFit)
confusionMatrix(test$Direction, predict(modelFit, test))
install.packages("ratte")
install.packages("rattle")
library(rattle)
data(wine)
head(wine)
wine.stand <- scale(wine[-1])
#cl <- round(sqrt(nrow(wine)/2))
#cl
# kmeans - divide in 3 clusters
k.means.fit <- kmeans(wine.stand, 3)
head(k.means.fit)
head(wine.stand)
wine.stand <- scale(wine[-1])
#cl <- round(sqrt(nrow(wine)/2))
#cl
# kmeans - divide in 3 clusters
k.means.fit <- kmeans(wine.stand, 3)
head(k.means.fit)
head(wine.stand)
head(k.means.fit)
head(k.means.fit)
attributes(k.means.fit)
k.means.fit$centers
k.means.fit$size
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")}
wssplot(wine.stand, nc=6)
install.packages("cluster")
library(cluster)
clusplot(wine.stand, k.means.fit$cluster, main='2D representation of the Cluster solution',
color=TRUE, shade=TRUE,
labels=2, lines=0)
table(wine[,1],k.means.fit$cluster)
d <- dist(wine.stand, method = "euclidean") # Euclidean distance matrix.
H.fit <- hclust(d, method="ward.D2")
plot(H.fit) # display dendogram
groups <- cutree(H.fit, k=3) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters
rect.hclust(H.fit, k=3, border="red")
table(wine[,1],groups)
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
library(factoextra)
fviz_cluster(k.means.fit, data = wine.stand,
palette = c("#2E9FDF", "#00AFBB", "#E7B800"),
geom = "point",
ellipse.type = "convex",
ggtheme = theme_bw()
)
#library "rattle" para utilizar o data set wines
library(rattle)
data(wine)
head(wine)
wine.stand <- scale(wine[-1])
head(wine.stand)
#cl <- round(sqrt(nrow(wine)/2))
#cl
# kmeans - divide in 3 clusters
k.means.fit <- kmeans(wine.stand, 3)
head(k.means.fit)
attributes(k.means.fit)
k.means.fit$centers
k.means.fit$cluster
k.means.fit$size
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")}
wssplot(wine.stand, nc=6)
library(cluster)
clusplot(wine.stand, k.means.fit$cluster, main='2D representation of the Cluster solution',
color=TRUE, shade=TRUE,
labels=2, lines=0)
table(wine[,1],k.means.fit$cluster)
d <- dist(wine.stand, method = "euclidean") # Euclidean distance matrix.
H.fit <- hclust(d, method="ward.D2")
library(factoextra)
fviz_cluster(k.means.fit, data = wine.stand,
palette = c("#2E9FDF", "#00AFBB", "#E7B800"),
geom = "point",
ellipse.type = "convex",
ggtheme = theme_bw()
)
plot(H.fit) # display dendogram
groups <- cutree(H.fit, k=3) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters
rect.hclust(H.fit, k=3, border="red")
table(wine[,1],groups)
url = 'http://www.biz.uiowa.edu/faculty/jledolter/DataMining/protein.csv'
food <- read.csv(url)
head(food)
set.seed(123456789) ## to fix the random starting clusters
grpMeat <- kmeans(food[,c("WhiteMeat","RedMeat")], centers=3, nstart=10)
grpMeat
## list of cluster assignments
o=order(grpMeat$cluster)
data.frame(food$Country[o],grpMeat$cluster[o])
plot(food$Red, food$White, type="n", xlim=c(3,19), xlab="Red Meat", ylab="White Meat")
text(x=food$Red, y=food$White, labels=food$Country,col=grpMeat$cluster+1)
set.seed(123456789)
grpProtein <- kmeans(food[,-1], centers=7, nstart=10)
o=order(grpProtein$cluster)
data.frame(food$Country[o],grpProtein$cluster[o])
library(cluster)
clusplot(food[,-1], grpProtein$cluster, main='2D representation of the Cluster solution', color=TRUE, shade=TRUE, labels=2, lines=0)
library(factoextra)
fviz_cluster(grpProtein, data = food[,-1],
palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#ADD8E6", "#DEB887", "#DA70D6", "#FF6347"),
geom = "point",
ellipse.type = "convex",
ggtheme = theme_bw()
)
#load file
path = "/media/angelica/Dados/Dev/GitHub/DataScience Courses/Kmeans/"
offers<-read.table("/media/angelica/Dados/Dev/GitHub/DataScience Courses/Kmeans/offers.csv", sep = ',', header=T)
library(factoextra)
fviz_cluster(grpProtein, data = food[,-1],
palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#ADD8E6", "#DEB887", "#DA70D6", "#FF6347"),
geom = "point",
ellipse.type = "convex",
ggtheme = theme_bw()
)
cluster.pivot<-melt(cluster.deals,id=c("Offer","Cluster"))
install.packages("rJava")
install.packages("NLP")
install.packages("openNLP")
install.packages("openNLPmodels.pt",
repos = "http://datacube.wu.ac.at/",
type = "source")
install.packages("RWeka")
install.packages("qdap")
bio <- readLines("data/nlp/anb-jarena-lee.txt")
bio <- readLines("data/nlp/anb-jarena-lee.txt")
library(rJava)
library(NLP)
library(openNLP)
library(RWeka)
bio <- readLines("/Volumes/Data HD/Data Science/GITHUB/Courses_DataScience/NLP/anb-jarena-lee")
file <- file.choose()
bio <- readLines(file)
print(bio)
bio <- paste(bio, collapse = " ")
print(bio)
install.packages("magrittr")
install.packages("magrittr")
library(NLP)
library(openNLP)
library(magrittr)
install.packages("magrittr")
library(NLP)
library(openNLP)
library(magrittr)
library("magrittr", lib.loc="~/opt/anaconda3/envs/Rstudio/lib/R/library")
library(NLP)
library(openNLP)
library(magrittr)
library(rJava)
library(NLP)
library(openNLP)
library(RWeka)
file <- file.choose()
bio <- readLines(file)
print(bio)
bio <- paste(bio, collapse = " ")
print(bio)
library(NLP)
library(openNLP)
library(magrittr)
bio <- as.String(bio)
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
library(NLP)
library(openNLP)
library(magrittr)
bio <- as.String(bio)
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
bio_annotations <- annotate(bio, list(sent_ann, word_ann))
install.packages("ggmap")
library(NLP)
library(openNLP)
library(magrittr)
bio <- as.String(bio)
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
bio_annotations <- annotate(bio, list(sent_ann, word_ann))
class(bio_annotations)
head(bio_annotations)
bio_doc <- AnnotatedPlainTextDocument(bio, bio_annotations)
bio_doc <- AnnotatedPlainTextDocument(bio, bio_annotations)
Agora podemos extrair informações de nosso documento usando funções de acesso como sents () para obter as frases e words () para obter as palavras. Poderíamos obter apenas o texto simples com as.character (bio_doc).
sents(bio_doc) %>% head(2)
words(bio_doc) %>% head(10)
person_ann <- Maxent_Entity_Annotator(kind = "person")
install.packages("openNLPmodels.en",
repos = "http://datacube.wu.ac.at/",
type = "source")
person_ann <- Maxent_Entity_Annotator(kind = "person")
location_ann <- Maxent_Entity_Annotator(kind = "location")
organization_ann <- Maxent_Entity_Annotator(kind = "organization")
pipeline <- list(sent_ann,
word_ann,
person_ann,
location_ann,
organization_ann)
bio_annotations <- annotate(bio, pipeline)
bio_doc <- AnnotatedPlainTextDocument(bio, bio_annotations)
entities <- function(doc, kind) {
s <- doc$content
a <- annotations(doc)[[1]]
if(hasArg(kind)) {
k <- sapply(a$features, `[[`, "kind")
s[a[k == kind]]
} else {
s[a[a$type == "entity"]]
}
}
entities(bio_doc, kind = "person")
# Extract entities from an AnnotatedPlainTextDocument
entities <- function(doc, kind) {
s <- doc$content
a <- annotation(doc)[[1]]
if(hasArg(kind)) {
k <- sapply(a$features, `[[`, "kind")
s[a[k == kind]]
} else {
s[a[a$type == "entity"]]
}
}
entities(bio_doc, kind = "person")
head(bio_doc)
entities <- function(doc, type) {
s <- doc$content
a <- annotation(doc)[[1]]
if(hasArg(type)) {
k <- sapply(a$features, `[[`, "type")
s[a[k == type]]
} else {
s[a[a$type == "entity"]]
}
}
entities <- function(doc, kind) {
s <- doc$content
a <- annotation(doc)[[1]]
if(hasArg(kind)) {
k <- sapply(a$features, `[[`, "kind")
s[a[k == kind]]
} else {
s[a[a$type == "entity"]]
}
}
entities(bio_doc, kind = "person")
entities(bio_doc, kind = "location")
entities(bio_doc, kind = "organization")
entities <- function(doc, kind) {
s <- doc$content
a <- annotation(doc)
if(hasArg(kind)) {
k <- sapply(a$features, `[[`, "kind")
s[a[k == kind]]
} else {
s[a[a$type == "entity"]]
}
}
entities(bio_doc, kind = "person")
entities(bio_doc, kind = "location")
entities(bio_doc, kind = "organization")
library(NLP)
library(openNLP)
library(magrittr)
filenames <- Sys.glob("data/itinerants/*.txt")
filenames
filenames <- Sys.glob("/Volumes/Data HD/Data Science/GITHUB/Courses_DataScience/NLP/*.txt")
filenames
texts <- filenames %>%
lapply(readLines) %>%
lapply(paste0, collapse = " ") %>%
lapply(as.String)
names(texts) <- basename(filenames)
str(texts, max.level = 1)
filenames <- Sys.glob("/Volumes/Data HD/Data Science/GITHUB/Courses_DataScience/NLP/*.txt")
filenames
texts <- filenames %>%
lapply(readLines) %>%
lapply(paste0, collapse = " ") %>%
lapply(as.String)
names(texts) <- basename(filenames)
str(texts, max.level = 1)
annotate_entities <- function(doc, annotation_pipeline) {
annotations <- annotate(doc, annotation_pipeline)
AnnotatedPlainTextDocument(doc, annotations)
}
itinerants_pipeline <- list(
Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator(),
Maxent_Entity_Annotator(kind = "person"),
Maxent_Entity_Annotator(kind = "location")
)
# We won't actually run this long-running process. Instead we will just load the
# cached results.
load("data/nlp-cache.rda")
# We won't actually run this long-running process. Instead we will just load the
# cached results.
load("/Volumes/Data HD/Data Science/GITHUB/Courses_DataScience/NLP/nlp-cache.rda")
texts_annotated <- texts %>%
lapply(annotate_entities, itinerants_pipeline)
texts_annotated <- texts %>%
lapply(annotate_entities, itinerants_pipeline)
